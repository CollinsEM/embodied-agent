PERCEPTION

Transducer: A device capable of translating a physical phenomenon or
material property into a continuous (analog) or discrete (digital)
electrical signal. This will correspond to the physical device on the
bot/agent that senses signals coming from its environment. 

Sensor: An array of transducers whose physical layout
(i.e. separation) is fixed. The output signal generated by each sensor
is a spatio-temporal signal which is continuous or discrete in time
and discrete in space. (Each transducer produces a discrete temporal
signal.)

The inputs to the sensor modules will be driven by interactions with
the environment and encoded by the transducers into usable
signals. The encoding of these signals will result in sparse
distributed representations (SDRs) that will be used as sensory inputs
into the cognitive model.

ACTION

From the top down, the action of the agent will be driven by an SDR
representing the motor output of the cognitive algorithm. This motor
SDR will then be converted into impulses that will drive the positions
of joint angles on the body of the agent in a realistic
(i.e. physically constrained) manner.

Motor output: SDRs representing directives to move articulated joints
on the agent's body. In a human, these would drive muscle contractions
which would then cause bones connected by joints to move in a fully
articulated manner. For now, we can probably skip the muscle movements
and just directly specify the joint angles with appropriate
constraints.

Joints: Connections between the 'skeletal bones' that were orignally
created for rigging the animated model. The motor output SDRs will
need to be converted into appropriate joint angles compatible with the
animation loop.

PHYSICAL INTERACTION

There will need to be a minimal physics engine running that can
correctly simulate gravity, friction, and surface collisions.
